
We review some popular procedures for signal detection and signal support recovery tasks in this section.

{\bf Signal detection.}
One of the commonly used statistics in sparse signal detection problems such as \eqref{eq:global-test-additive} and \eqref{eq:global-test-chisq} are the $L_q$ norms of the observations $x$,
\begin{equation} \label{eq:Lq-norm}
   L_q(x) = \left(\sum_{i=1}^p{|x(i)|^q}\right)^{1/q}.
\end{equation}
Typical choices of $q$ include $q=1, 2$ and $\infty$, where $L_\infty(x)$ is interpreted as the limit of $L_q(x)$ norms as $q\to\infty$, and is equivalent to $\max_{i}|x(i)|$.
Test procedures based on \eqref{eq:Lq-norm} may then be written as $T(\mathcal{R}(x)) = \mathbbm{1}_{(t,+\infty)}(L_q(x))$, where the cutoff $t$ can be chosen to control the Type I error at desired levels.

While \eqref{eq:Lq-norm} measures the deviation of the data from the origin in an omnidirectional manner, statistics that are tailored to the alternatives can be used in the hopes of power improvement if the directions of the alternatives are known.
For example, if we were to test positive mean shifts in the additive error model \eqref{eq:model-additive} in the one-sided alternative,
\begin{equation} \label{eq:global-test-one-sided}
    %\mathcal{H}_0: \mu(i) = 0, \;i=1,\ldots,p,
    %\quad\text{v.s.}\quad 
    \mathcal{H}_1: \mu(i)> 0, \; \text{for some }i\in\{1,\ldots,p\},
\end{equation}
one might consider monitoring the sum (or equivalently, the arithmetic average) of the observations, 
\begin{equation} \label{eq:sum-statistic}
    S(x) := \sum_{i=1}^p{x(i)},
\end{equation}
or the maximum of the observations,
\begin{equation} \label{eq:max-statistic}
    M(x) := \max_{i=1,\ldots,p}{x(i)}.
\end{equation}
Other tests based on the empirical \ac{CDF} are also available.
Assuming the same one-sided alternative, let 
\begin{equation}
    p(i) = 1 - \sup\{F_i(y)\,:\,y<x(i)\}, \quad i = 1,\ldots,p,
\end{equation}
be the p-values of the individual observations, where $F_i$ is the \ac{CDF} of the $i$-th component $x(i)$ under $\mathcal{H}_0$.
We define empirical \ac{CDF} of the p-values as
\begin{equation}
    \widehat{F}_p(t) = 
    \frac{1}{p} \sum_{i=1}^p \mathbbm{1}_{(-\infty, t]}(p(i)).
\end{equation}
Viewed as random elements in the space of c\`adl\`ag functions with the Skorohod $J_1$ topology, the centered and scaled \ac{CDF}s converge weakly to a Brownian bridge,
\begin{equation*}
    \left\{\sqrt{p}\left(\widehat{F}_p(t) - t\right)\right\}_{t\in[0,1]} 
    % \stackrel{J_1}{\longrightarrow} 
    \implies
    \left\{\mathbb{B}(t)\right\}_{t\in[0,1]},
\end{equation*}
under the global null $\mathcal{H}_0$ and mild continuity assumptions on the $F_i$'s \citep{skorokhod1956limit}. 
Therefore, goodness-of-fit statistics such as Kolmogorov-Smirnov distance \citep{smirnov1948table}, Cramer-von Mises-type statistics \citep{cramer1928composition, anderson1952asymptotic} that measure the departure from this limiting behavior can be used for testing $\mathcal{H}_0$ against $\mathcal{H}_1$.
Of particular interest is the \ac{HC} statistic, first proposed by \cite{tukey1976lecture},
\begin{equation*} %\label{eq:HC-statistic}
    HC(x) = 
    \max_{0\le t\le\alpha_0}\frac{\widehat{F}_p(t) - t}{\sqrt{t(1 - t)/p}}.
\end{equation*}

Performance of these statistics in high-dimensional sparse signal detection problems will be reviewed in Section \ref{sec:asymptotics}, and analyzed in Chapter \ref{chap:phase-transitions}.

\medskip

{\bf Signal support recovery.}
In signal support recovery tasks, we shall study the performance of five procedures, all of which belong to the broad class of thresholding procedures.
\begin{definition}[Thresholding procedures]
A thresholding procedure for estimating the support 
$S:=\{i\, :\, \lambda(i)\neq0\}$ is one that takes on the form
\begin{equation} \label{eq:thresholding-procedure}
    \widehat{S} = \left\{i\,|\,x(i) \ge t(x)\right\},
\end{equation}
where the threshold $t(x)$ may depend on the data $x$.
\end{definition}

Examples of thresholding procedures include ones that aim to control FWER \eqref{eq:FWER-FWNR} --- Bonferroni's \cite{dunn1961multiple}, Sid\'ak's \citep{vsidak1967rectangular}, Holm's \citep{holm1979simple}, and Hochberg's procedure \citep{hochberg1988sharper} --- as well as procedures that target FDR \eqref{eq:FDR-FNR}, such as the Benjamini-Hochberg \cite{benjamini1995controlling} and the Cand\'es-Barber procedure \cite{barber2015controlling}.
Indeed, the class of thresholding procedures \eqref{eq:thresholding-procedure} is so general that it contains most (but not all) of the statistical procedures in the multiple testing literature.
% \cite{roquain2011type}.


% \subsection{FWER-controlling procedures}
% \label{subsec:FWER-controlling-procedures}

We review five thresholding procedures, starting with the well-known Bonferroni's procedure which aims at controlling family-wise error rates.
\begin{definition}[Bonferroni's procedure]
Suppose the errors $\epsilon(i)$'s have a common marginal distribution $F$, Bonferroni's procedure with level $\alpha$ is the thresholding procedure that uses the threshold
\begin{equation} \label{eq:Bonferroni-procedure}
    t_p = F^{\leftarrow}(1 - \alpha/p).
\end{equation}
where  $F^{\leftarrow}(u)=\inf{\left\{x:F(x)\ge u\right\}}$ is the generalized inverse function.
\end{definition}
% It is easy to see that the family-wise error rate (FWER) is controlled at level $\alpha$ by applying the union bound, regardless of the error-dependence structure (see e.g.\ Relation \eqref{eq:Bonferroni-FWER-control}, below).
The Bonferroni procedure is deterministic (i.e., non data-dependent), and only depends on the dimension of the problem and the null distribution.
A closely related procedure is Sid\'ak's procedure \citep{vsidak1967rectangular},
which is a more aggressive (and also deterministic) thresholding procedure that uses the threshold
\begin{equation} \label{eq:Sidak-procedure}
    t_p = F^{\leftarrow}((1 - \alpha)^{1/p}).
\end{equation}
% can be shown to control FWER in the case independent errors.

The third procedure, strictly more powerful than Bonferroni's, is the so-called Holm's procedure \citep{holm1979simple}.
On observing the data $x$, its coordinates can be ordered from largest to smallest
$x(i_1) \ge x(i_2)  \ge \ldots \ge x(i_p)$,
where $(i_1, \ldots, i_p)$ is a permutation of $\{1, \ldots, p\}$. 
Denote the order statistics as $x_{[1]}, x_{[2]}, \ldots, x_{[p]}$.
\begin{definition}[Holm's procedure]
Let $i^*$ be the largest index such that
$$
\overline{F}(x_{[i]}) \le \alpha / (p-i+1),\quad \text{for all }\;i\le i^*.
$$
Holm's procedure with level $\alpha$ is the thresholding procedure with threshold
\begin{equation} \label{eq:Holm-procedure}
    t_p(x) = x_{[i^*]}.
\end{equation}
\end{definition}
In contrast to the Bonferroni procedure, Holm's procedure is data-dependent.
% It can be shown that Holm's procedure also controls FWER at $\alpha$ level, regardless of dependence in the data.
A closely related, more aggressive (data-dependent) thresholding procedure is Hochberg's procedure \citep{hochberg1988sharper}
%\begin{definition}[Hochberg's procedure]
%Hochberg's procedure 
which replaces the index $i^*$ in Holm's procedure with the largest index such that
$$
\overline{F}(x_{[i]}) \le \alpha / (p-i+1).
$$
Notice that both Holm's procedure and Hochberg's procedure compare p-values to the same thresholds $\alpha / (p-i+1)$.
However, Holm's procedure only rejects the set of hypotheses whose p-values are all smaller than their respective thresholds.
On the other hand, Hochberg's procedure rejects the set of hypotheses as long as the largest of their p-values fall below its threshold, and therefore, can be more powerful than Holm's procedure. 
%where  $\overline{F}(x)=1-F(x)$ is the survival function.
%\end{definition}

It can be shown that Bonferroni's procedure and Holm's procedure both control FWER at their nominal levels, regardless of dependence in the data \citep{holm1979simple}.
In contrast, Sid\'ak's procedure and Hochberg's procedure control FWER at nominal levels when data are independent \citep{vsidak1967rectangular, hochberg1988sharper}.

Last but not least, we review the \ac{BH} procedure, which aims at controlling \ac{FDR} in \eqref{eq:FDR-FNR}, proposed by  \cite{benjamini1995controlling}.

Recall the order statistics of our observations $x_{[1]} \ge x_{[2]}  \ge \ldots \ge x_{[p]}$.
\begin{definition}[Benjamini-Hochberg's procedure]
Let $i^*$ be the largest index such that
$$
\overline{F}(x_{[i]}) \le \alpha i/p.
$$
The Benjamini-Hochberg procedure with level $\alpha$ is the thresholding procedure with threshold
\begin{equation} \label{eq:BH-procedure}
    t_p(x) = x_{[i^*]},
\end{equation}
\end{definition}
The \ac{BH} procedure is shown to control the FDR at level $\alpha$ when the $x(i)$'s are independent \citep{benjamini1995controlling}.
Variations of the \ac{BH} procedure have been proposed to control the \ac{FDR} under dependent observations \citep{benjamini2001control}.

% We now turn to discuss the framework for anlayzing the asymptotic risks in high dimensions.

Performance of these procedures in high-dimensional sparse signal support recovery problems will be reviewed in Section \ref{sec:asymptotics}, and analyzed in Chapters \ref{chap:phase-transitions}, \ref{chap:exact-support-recovery}, and \ref{chap:GWAS}.
