
The notion of uniform relative stability (URS) in Definition \ref{def:URS} is the key to the necessary conditions for exact support recovery established in Theorem \ref{thm:necessary}.
In this chapter, we provide a complete characterization of the class of URS Gaussian arrays in terms of a simple condition on their covariance structure.
The condition is as follows.

% The proof of the main result is deferred until Section \ref{sec:proofs}.

% We define the class of dependence structures, referred to as uniformly decreasing dependence (UDD), for Gaussian arrays via their covariance matrices.

\begin{definition}[Uniformly decreasing dependence (UDD)] \label{def:UDD}
Consider a triangular array of jointly Gaussian distributed errors 
${\cal E} = \left\{\left(\epsilon_p(i)\right)_{i=1}^p, p = 1,2,\ldots\right\}$ 
with unit variances,
$$
\epsilon_p \sim \text{N}(0, \Sigma_p), \quad p=1,2,\ldots.
$$
The array ${\cal E}$ is said to be uniform decreasingly dependent (UDD) if 
for every $\delta>0$ there exists a finite $N(\delta)<\infty$, such that for every $i\in\{1,\ldots,p\}$, and $p\in\N$, we have
\begin{equation} \label{eq:UDD-definition}
    \Big|\left\{k\in\{1,\ldots,p\}:\Sigma_p(i,k)>\delta\right\}\Big| \le N(\delta)\quad \text{for all  } \delta>0.
\end{equation}
\end{definition}
That is, for every coordinate $i$, the number of elements which are more than $\delta$-correlated with $\epsilon_p(i)$ does not exceed $N(\delta)$. 

Note that the bound in \eqref{eq:UDD-definition} holds uniformly in $i$ and $p$, and only depends on $\delta$.
Also observe that on the left-hand side of \eqref{eq:UDD-definition}, we merely count in each row of $\Sigma_p$ the number of exceedances of covariances (not their absolute values!) over level $\delta$.

\begin{remark} \label{rmk:choice-of-N(delta)}
Without loss of generality, we may require that $N(\delta)$ be a monotone non-increasing function of $\delta$, for we can take
$$
N(\delta) = \sup_{p,i} \Big|\{k:\Sigma_p(i,k)>\delta\}\Big|,
$$
which is non-increasing in $\delta$.
Definition \ref{def:UDD} therefore states that the array is UDD when $N(\delta)<\infty$ for all $\delta>0$.
\end{remark}


Observe that the UDD condition does not depend on the order of the coordinates in the error 
vector $\epsilon_p = (\epsilon_p(i))_{i=1}^p$.  Often times, however, the errors are thought of 
coming from a stochastic process indexed by time or space.  To illustrate the generality of the 
UDD condition, we formulate next a simple sufficient condition (UDD$^\prime$) that is easier to 
check in a time-series context.

\begin{definition}[UDD\,$^\prime$]\label{d:UDD-prime}
For $\epsilon_p \sim \mathrm{N}(0,\Sigma_p)$ with unit variances, an array ${\cal E} = \left(\epsilon_p(i)\right)_{i=1}^p$ is said to satisfy the UDD\,$^\prime$ condition if there 
exist:
\begin{enumerate}
    \item[(i)] permutations $l_p$ of $\{1,\ldots,p\}$, for all $p\in\N$, and
    \item[(ii)] a non-negative sequence $(r_n)_{n=1}^\infty$ converging to zero $r_n\to 0$, as $n\to\infty$,
\end{enumerate}
such that 
\begin{equation} \label{eq:weak-correlation}
    \sup_{p\in\N} |\Sigma_p\left(i',j'\right)| \le r_{|i-j|}.
\end{equation}
where $i' = l_p(i)$, $j' = l_p(j)$, for all $i,j\in\{1,\ldots,p\}$.
\end{definition}

\begin{remark}
Without loss of generality, we may also require that $r_n$ be non-increasing in $n$, for we can replace $r_n$ with the non-increasing sequence $r'_n = \sup_{m\ge n} r_m$.
\end{remark}

\begin{proposition} \label{prop:UDD-equivalent}
UDD\,$^\prime$ implies UDD.
\end{proposition} 

\begin{proof}%[Proof of Proposition \ref{prop:UDD-equivalent}]
%The functions $N(\delta)$ and $r(n)=r_n$ are inverses of each other.
% {\bf $\text{UDD} \implies \text{UDD'}$:}
% If $N(0)<\infty$, then we can take 
% $$
% r_n = \underbrace{1, \ldots, 1}_{\lfloor N(0)/2\rfloor + 1}, 0, 0, \ldots.
% $$
% and recursively construct the permutations as follows.
% Start with any element

% We proof the contrapositive. 
% Suppose for all permutations and any sequence $r_n\to0$, there exists $i,j$ such that $\Sigma(i',j')>r_{|i'-j'|}$,
% then for any $\delta>0$ and any finite $M<\infty$, we can take $r_n$ to be a sequence of $M$ 1's followed by a $\delta$, i.e., 
% $$
% r_n = \underbrace{1, \ldots, 1}_{M+1}, \delta, \ldots.
% $$
% \fbox{Not true:}
% However, since there exists $i',j'$ such that $\Sigma(i',j')>r_{|i'-j'|}$, the set 
% $$
% S = l^{-1}\left(\left\{j',i'-N,\ldots,i'-1,i',i'+1,\ldots,i'+N\right\}\right)
% $$ 

% {\bf $\text{UDD'} \implies \text{UDD}$:} 
Since $r_n\to 0$, for any $\delta > 0$, there exists an integer 
$M = M(\delta)<\infty$ such that $r_n\le\delta$, for all $n\ge M$. 
Thus, by \eqref{eq:weak-correlation}, for every fixed 
$j' \in\{1,\ldots,p\}$, we can have $|\cov(\epsilon_p(k'),\epsilon_p(j'))| > \delta$,
only if $k'$ belongs to the set:
$$ 
 \left\{ k' \in \{1,\dots,p\} \, :\, j-M \le  k := l_p^{-1}(k') \le j+M \right\},
$$
where $j:= l_p^{-1}(j')$. That is, there are at most $2M+1<\infty$ indices  $k'\in\{1,\dots,p\}$, whose covariances with $\epsilon(j')$ may exceed $\delta$. 
Since this holds uniformly in $j'\in\{1,\ldots,p\}$, Condition UDD follows with 
$N(\delta) = 2M+1$.
\end{proof}



% \begin{definition}[Uniformly decreasing dependence (UDD)] \label{def:weak-dependence}
% Consider a triangular array of jointly Gaussian distributed errors $\left(\epsilon_p(j)\right)_{j=1}^p$ with unit variances, $\epsilon_p \sim \mathcal N(0,\Sigma_p)$. The array ${\cal E}$ is said to be uniform decreasingly dependent (UDD) with rate $(r_n)_{n=1}^\infty$ if 
% \begin{equation} \label{eq:weak-correlation}
%     \sup_p |\Sigma_p(i,j)| \le r_{|i-j|}
% \end{equation}
% such that $r_n\to 0$, as $n\to\infty$.
% \end{definition}
% 
% \begin{remark} \label{rmk:UDD-equivalent}
% In situations where there is no natural ordering of the components, it is sufficient that a permutation of the vector $\epsilon$ in its coordinates satisfy the requirements above. 
% In fact, the UDD condition can be equivalently stated as follows: for any $\delta>0$, and any coordinate $j\in\{1,\ldots,p\}$, there are at most $N(\delta)<\infty$ coordinates whose covariances with $\epsilon(j)$ exceed $\delta$; here $N(\delta)$ is a deterministic function independent of $p$.
% $N(\delta) \to 1$ as $\delta \to 0$.
% \end{remark}

We now state the main result of this chapter.  It states that a Gaussian array is URS if and only if it is UDD.
The URS condition essentially requires that the dependencies decay in a uniform fashion, the rate at which dependence decay 
does \emph{not} matter.

\begin{theorem} \label{thm:Gaussian-weak-dependence}
Let ${\cal E}$ be a Gaussian triangular array with standard normal marginals.  
The array ${\cal E}$ has uniformly relatively stable (URS) maxima if and only if it is uniformly decreasing dependent (UDD).
\end{theorem}

% The proof of Theorem \ref{thm:Gaussian-weak-dependence} is given in Section \ref{sec:proofs}. 

Specifically, for stationary Gaussian arrays, we have the following corollary.

\begin{corollary} \label{cor:stationary-Gaussian-errors}
Let ${\cal E} = \{\epsilon_p(i) = Z(i)\}$ for a stationary Gaussian time series ${\cal Z} = \{Z(i)\}$.
Then $\cal E$ is {URS} if and only if the autocovariance function $\cov(Z(k), Z(0))\to 0$, as $k\to\infty$.
\end{corollary}

Corollary \ref{cor:stationary-Gaussian-errors} follows by Theorem \ref{thm:Gaussian-weak-dependence} and the observation that UDD is equivalent to vanishing autocovariance of $\cal Z$.
A slightly weaker form of the ``if'' part was established in Theorem 3 of \cite{berman1964limit}.

Returning again to the study of support recovery problems, Theorem \ref{thm:Gaussian-weak-dependence} and the necessary condition for exact support recovery in Theorem \ref{thm:necessary} yield the following result.

\begin{corollary} \label{cor:weakly-dependent-errors}
For UDD Gaussian errors, the result in Theorem \ref{thm:necessary} holds.
\end{corollary}

One may ask, whether the UDD (equivalently, URS) condition can be relaxed further for the phase-transition result in Theorem \ref{thm:necessary} to 
hold.  As a counterpart to Remark \ref{rmk:dependence-assumptions}, we demonstrate next that the dependence conditions in Theorem \ref{thm:necessary} 
are nearly optimal.  Specifically, we show that if the URS dependence condition is violated, then it may be possible to recover the support 
of weaker signals, falling below the boundary.   The main idea is to use the equivalence of URS and UDD to construct a Gaussian error array,
whose correlations do not decay in a uniform fashion (UDD fails). As we will see, in such a case one can do substantially better in terms of 
support recovery.  This shows that the URS condition is nearly optimal in the Gaussian setting.  
Numerical simulations illustrating this example can be found in Section \ref{suppsec:numerical}, below. 

\begin{example}[On the tightness of the URS condition for exact support recovery] \label{exmp:counter-example}
Suppose ${\cal E} = \left(\epsilon_p(i)\right)_{i=1}^p$ is Gaussian, and is comprised of $\lfloor p^{1-\beta}\rfloor$ blocks, each of size at least $\lfloor p^\beta \rfloor$.  Let the elements within each block have correlation 1, and let the elements from different blocks be independent. 
If $\underline{r} \ge 4(1-\beta)$, then the procedure 
$$
 \widehat{S} = \big\{i:x(i)>\sqrt{2(1-\beta)\log{p}}\big\}
 $$ 
yields exact support recovery, i.e., $\mathbb P[\widehat{S} = S] \to 1$, as $p\to\infty$.  This requirement on the signal size is strictly 
weaker than that of the strong classification boundary, since $4(1-\beta) < (1 + \sqrt{1-\beta})^2$ on $\beta\in(0,1)$.
\end{example} 
\begin{proof}[Example \ref{exmp:counter-example}]
Let $t_p^* = \sqrt{2(1-\beta)\log{p}}$  and observe that $\widehat{S} = \{j:x(j)>t_p^*\}$.
Analogous to \eqref{eq:Bonferroni-FWER-control} in the proof of Theorem \ref{thm:sufficient}, we have
\begin{align*}
    \P\left[\widehat{S} \subseteq S\right] 
        &= 1 - \P\left[\max_{j\in S^c}x(j) > t_p^*\right] 
        = 1 - \P\left[\max_{j\in S^c}\epsilon(j) > t_p^*\right] \nonumber \\
      % \ge 1 - \P\left[\max_{j\in\{1,\ldots,p\}}\epsilon(j) > t_p\right] \nonumber \\
        &\ge 1 - \P\left[\max_{j\in\{1,\ldots,p\}}\epsilon(j) > t_p^*\right] 
        \ge 1 - \P\left[\max_{j\in\{1,\ldots,\lfloor p^{1-\beta}\rfloor\}}\widetilde{\epsilon}(j) > t_p^*\right]
\end{align*}
where $\left(\widetilde{\epsilon}\right)_{j=1}^{\lfloor p^{1-\beta}\rfloor}$'s are independent Gaussian errors; in the last inequality we used the assumption that there are at most $\lfloor p^{1-\beta}\rfloor$ independently distributed Gaussian errors in $\left(\epsilon_p(j)\right)_{j=1}^p$.
By Example \ref{exmp:FWER-controlling_procedures} (with $\lfloor p^{1-\beta}\rfloor$ taking the role of $p$), we know that the FWER goes to 0 at a rate of 
$\left(2\log{\lfloor p^{1-\beta}\rfloor}\right)^{-1/2}$.
Therefore, the probability of no false inclusion converges to 1.


On the other hand, since the signal sizes are no smaller than $(\nu\underline{r}\log p)^{1/\nu} = \sqrt{2\underline{r}\log p}$ (for $\nu = 2$), similar to 
\eqref{eq:sufficient-proof-eq1}, we obtain
\begin{align}
    \P\left[\widehat{S} \supseteq S\right] 
    &\ge \P\left[\min_{j\in S}\epsilon(j) > \sqrt{2(1-\beta)\log{p}} - \sqrt{2\underline{r}\log{p}} \right] \nonumber \\
    &= \P\left[\max_{j\in S}\left(-\epsilon(j)\right) < \sqrt{2\log{p}}\left(\sqrt{\underline{r}}-\sqrt{1-\beta}\right) \right] \nonumber \\
    &= \P\left[\frac{\max_{j\in S}(-\epsilon(j))}{u_{|S|}} < \frac{\sqrt{\underline{r}}-\sqrt{1-\beta}}{\sqrt{1-\beta}}\left(1+o(1)\right) \right], \label{eq:sufficient-proof-counter-example}
\end{align}
where in the last line we used the quantiles \eqref{eq:AGG-quantiles}.
Since the minimum signal size is bounded below by $\underline{r} > 4(1-\beta)$, the right-hand-side of the inequality in \eqref{eq:sufficient-proof-counter-example} converges to a constant strictly larger than 1. While the left-hand-side, by Slepian's lemma (recall 
Theorem \ref{thm:Slepian-lemma} and Relation \ref{e:Slepian-lemma-maxima}), is stochastically smaller than a r.v.\ going to 1.  Namely, we have
\begin{equation}
  \frac{1}{u_{|S|}} \max_{j\in S}(-\epsilon(j)) \stackrel{d}{\le} \frac{1}{u_{|S|}} \max_{j\in S} \epsilon^*(j) \stackrel{\P}{\longrightarrow} 1,
\end{equation}
where $\left({\epsilon^*}\right)_{j=1}^{\lfloor p^{1-\beta}\rfloor}$'s are independent Gaussian errors.
Therefore the probability in \eqref{eq:sufficient-proof-counter-example} must also converge to 1.
\end{proof}




\medskip

Before proceeding to the proof of Theorem \ref{thm:Gaussian-weak-dependence}, we will briefly discuss 
the relationships between UDD and other dependence conditions in the context of extreme value theory. The main idea
we would like to convey is that UDD (and equivalently URS) is an exceptionally mild condition on the dependence of the array.

\medskip
\noindent{\bf The Berman and UDD conditions.} 
 \label{UDD_and_Berman}
 Suppose that the array of errors  ${\cal E}$ comes from a stationary Gaussian time series $\epsilon(i),\ i\in \mathbb{N}$, with auto-covariance $r_p=\cov(\epsilon(i+p),\epsilon(i))$. 
One is interested in the asymptotic behavior of the maxima $M_p:=\max_{i=1,\dots,p} \epsilon(i)$.

In this setting, the Berman's condition, introduced in \cite{berman1964limit}, requires that
\begin{equation} \label{eq:Berman}
    r_p \log p \to 0,\ \ \mbox{ as }p\to\infty.
\end{equation}
This condition entails that 
\begin{equation}
    \label{eq:Gauss-max-in-distribution}
  a_p (M_p - b_p) \stackrel{d}{\longrightarrow } Z,\  \ \mbox{ as }p\to\infty,
\end{equation}
with the Gumbel limit distribution $\mathbb P [Z\le x] = \exp\{-e^{-x}\},\ x\in \mathbb R$, 
where 
$$
a_p = \sqrt{2\log p},\quad b_p  = \sqrt{2\log p} - \frac{1}{2}\left(\sqrt{2\log p}\right)^{-1}\left(\log \log (p) + \log(4\pi)\right),
$$ 
are {\em the same} centering and normalization sequences
as in the case of iid $\epsilon(i)$'s.  
Berman's condition is one of the weakest dependence conditions  in the literature for which the convergence in \eqref{eq:Gauss-max-in-distribution} holds. 
See, e.g., Theorem 4.4.8 in \cite{embrechts2013modelling}, where \eqref{eq:Berman} is described as ``very weak''.

Instances where the dependence in the time series is so strong that Berman's condition \eqref{eq:Berman} fails have also been studied.  In such 
cases, one may continue to have \eqref{eq:Gauss-max-in-distribution} but typically the sequences of normalizing and centering constants will be
{\em different} from the iid case, and the corresponding limit is usually no longer Gumbel; see, for example, Theorems 6.5.1 and  6.6.4 in \cite{leadbetter2012extremes}, and \cite{mccormick1976weak}. 
% In particular, \cite{mccormick1976weak} derived the normalizing constants when both $r_p\to 0$ monotonically and  $r_p \log p \to \infty$ monotonically, as $p\to\infty$. 
% In this case, convergence in distribution still takes place, with the maxima concentrating along a sequence asymptotic to \eqref{eq:quantiles}.

In our high dimensional support estimation context, the notion of relative stability is sufficient and more natural than the finer notions of distributional convergence.
If one is merely interested in the asymptotic relative stability of the Gaussian maxima, then Berman's condition can be relaxed significantly 
\citep[see also, Theorem 4.1 of][]{berman1964limit}.  Observe that by Proposition \ref{prop:UDD-equivalent},  the Berman condition \eqref{eq:Berman} implies UDD and hence relative stability (Theorem \ref{thm:Gaussian-weak-dependence}), i.e., 
\begin{equation} \label{eq:Gaussian-URS}
  \frac{1}{b_p} M_p \stackrel{\mathbb P}{\to} 1,\quad\mbox{as}\quad p\to\infty.
\end{equation}
This {\em concentration of maxima} property can be readily deduced from \eqref{eq:Gauss-max-in-distribution}, since $a_p b_p \sim 2\log(p) \to \infty$ as $p\to\infty$.
Theorem \ref{thm:Gaussian-weak-dependence} shows that \eqref{eq:Gaussian-URS} holds if the much weaker uniform dependence condition UDD holds. 
Note that our condition is coordinate free --- neither monotonicity of the sequence $r_p$ nor stationarity of the underlying array is required. This
makes it substantially broader than the time series setting in the seminal work \cite{berman1964limit}.

% The method of proof is also very different from the results on distributional convergence in the references mentioned above. 

\medskip

The rest of this chapter is devoted to the proof of the main result, i.e., Theorem \ref{thm:Gaussian-weak-dependence}. 
We first introduce a key lemma regarding the structure of an {\em arbitrary} correlation matrix of high-dimensional random variables.
The proof uses a surprising, yet elegant application of Ramsey's Theorem from the study of combinatorics.
%; this application, and its consequences in high-dimensional probability, are presented in Section \ref{subsec:Ramsey}.
The `only if' part of Theorem \ref{thm:Gaussian-weak-dependence} follows from this lemma, in Section \ref{sec:URS=>UDD}. 

The proof of the `if' part is detailed in Section \ref{sec:UDD=>URS}.
The arguments there have been recently extended to establish bounds on the rate of concentration of maxima in \cite{kartsioukas2019rate}; see also, \cite{tanguy2015some} and the related notion of super-concentration of \cite{chatterjee2014superconcentration}.

\section{Ramsey's  theory and the structure of correlation matrices} 
\label{sec:Ramsey}
\input{6.URS/6.proofs-Ramsey.tex}

\section{URS implies UDD (Proof of the `only if' part of Theorem \ref{thm:Gaussian-weak-dependence})} 
\label{sec:URS=>UDD}
\input{6.URS/6.proofs-URS-UDD.tex}

\section{UDD implies URS (Proof of the `if' part of Theorem \ref{thm:Gaussian-weak-dependence})} 
\label{sec:UDD=>URS}
\input{6.URS/6.proofs-UDD-URS.tex}

\section{Numerical illustrations of exact support recovery under dependence}
\label{sec:URS-numerical}
\input{6.URS/6.numerical.tex}
