\subsection{Proofs of Propositions \ref{prop:optimal-oracle-procedures} and \ref{prop:likelihood-ratio-thresholding}}

\begin{proof}[Proof of Proposition \ref{prop:optimal-oracle-procedures}]
The problem of support recovery can be equivalently stated as a classification problem, where the discrete parameter space is $\mathcal{S} = \{S\subseteq\{1,\ldots,p\}:|S|=s\}$, and the observation $x \in\R^p$ has likelihood $f(x|S)$ indexed by the support set $S$.

By the optimality of the Bayes classifier (see, e.g., \citep{domingos1997optimality}), a set estimator that maximizes the probability of support recovery is such that
$$
\widehat{S} \in \argmax_{S\in \mathcal{S}} f(x|S) \pi(S).
$$
Since we know from \eqref{eq:uniform} that $\pi(S)$ is uniform, the problem in our context reduces to showing that $f(x|\widehat{S}^*) = f(x|\widehat{S})$, where $f(x|S)$ is the conditional distribution of data given the unordered support $S$,
$$
f(x|S) 
= \sum_{P\in\sigma(S)} f(x|P) \pi^{\text{ord}}(P|S) 
= \frac{1}{s!} \left(\sum_{P\in\sigma{(S)}} \prod_{i=1}^s {f_{i}(x(P(i)))}\right) \prod_{k\not\in S}{f_0(x(k))},
$$
where $\sigma(S)$ is the set of all permutations of the indices in the support set $S$.

Suppose $\widehat{S} \neq \widehat{S}^*$, then there must be indices $j \in \widehat{S}$ and $j' \in \widehat{S}^c$ such that $x(j) \le x(j')$.
We exchange the labels of $x(j)$ and $x(j')$, and form a new estimate $\widehat{S}\,' = \big(\widehat{S}\setminus\{j\}\big)\cup\{j'\}$.
Comparing the likelihoods under $\widehat{S}$ and $\widehat{S}\,'$, we have
\begin{align}
    f(x|\widehat{S}) - f(x|\widehat{S}\,') 
    &= \frac{1}{s!} \sum_{P\in\sigma{(\widehat{S})}} \prod_{i=1}^s {f_{i}(x(P(i)))} f_0(x(j'))\prod_{k\not\in \widehat{S}\cup\{j'\}}{f_0(x(k))} - \nonumber \\
    &\quad\quad\quad - \frac{1}{s!} \sum_{P'\in\sigma{(\widehat{S}')}} \prod_{i=1}^s {f_{i}(x(P'(i)))} f_0(x(j)) \prod_{k\not\in \widehat{S}'\cup\{j\}}{f_0(x(k))} \nonumber \\
    &= \frac{1}{s!} \left(\sum_{i=1}^s a_i  \Big(f_i(x(j)) f_0(x(j')) - f_i(x(j')) f_0(x(j))\Big) \right) \prod_{k\not\in \widehat{S}\cup\{j'\}}{f_0(x(k))}, \label{eq:MLR-optimality-proof}
    % = \log{f_\delta(x(j))} + \log{f_0(x(j'))} - \log{f_\delta(x(j'))} - \log{f_0(x(j))} 
\end{align}
where the last equality follows by first summing over all permutations fixing $P(i) = j$ and $P'(i) = j'$, and setting $a_i = \sum_{P\in\sigma{(\widehat{S}\setminus\{j\})}} \prod_{i'\neq i} {f_{i'} (x(P(i')))}$. Notice that the $a_i$'s are non-negative.

Since $x(j) \le x(j')$, and that each of $\{f_0, f_{i}\}$ is an MLR family, we have
$$
\frac{f_i(x(j))}{f_0(x(j))} - \frac{f_i(x(j'))}{f_0(x(j'))} \le 0 \implies f_i(x(j)) f_0(x(j')) - f_i(x(j')) f_0(x(j)) \le 0.
$$
Using Relation \eqref{eq:MLR-optimality-proof}, we conclude that $f(x|\widehat{S}) \le f(x|\widehat{S}\,')$.
This implies that any estimator that is not $\widehat{S}^*$ may be improved, and the optimality follows.
\end{proof}


\begin{lemma} \label{lemma:four-point-concavity}
Let $\phi$ be any concave function on $\R$. For any $x<y\in\R$, and $\delta>0$ we have
$$
\phi(x) + \phi(y+\delta) \le \phi(y) + \phi(x+\delta).
$$
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:four-point-concavity}]
Pick $\lambda = \delta/(y-x+\delta)$, by concavity of $f$ we have
\begin{equation} \label{eq:four-point-concavity-1}
    \lambda \phi(x) + (1-\lambda) \phi(y+\delta) 
    \le \phi(\lambda x + (1-\lambda)(y+\delta)) 
    = \phi(y),
\end{equation}
and
\begin{equation} \label{eq:four-point-concavity-2}
    (1-\lambda) \phi(x) + \lambda \phi(y+\delta)
    \le \phi((1-\lambda) x + \lambda(y+\delta)) 
    = \phi(x+\delta).
\end{equation}
Summing up \eqref{eq:four-point-concavity-1} and \eqref{eq:four-point-concavity-2} and we arrive at the conclusion as desired.
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop:likelihood-ratio-thresholding}]
The proof is entirely analogous to that of Proposition \ref{prop:optimal-oracle-procedures}.
Since we know from \eqref{eq:uniform} that $\pi(S)$ is uniform, the problem reduces to showing that $f(x|\widehat{S}_{\text{opt}}) = f(x|\widehat{S})$, where 
$$
\widehat{S} \in \argmax_{S \in \mathcal{S}} f(x|S) \pi(S).
$$
and
$f(x|S)$ is the conditional distribution of data given the unordered support $S$,
\begin{equation} \label{eq:likelihood-ratio-thresholding-proof}
    f(x|S) = \sum_P f(x|P) \pi^{\text{ord}}(P|S) = \prod_{j\in S} f_a(x(j)) \prod_{j\not\in S}{f_0(x(j))}.
\end{equation}
Suppose $\widehat{S} \neq \widehat{S}_{\text{opt}}$, then there must be indices $j \in \widehat{S}$ and $j' \in \widehat{S}^c$ such that $L(j) \le L(j')$.
If we exchange the labels of $L(j)$ and $L(j')$, that is, we form a new estimate $\widehat{S}\,' = \big(\widehat{S}\setminus\{j\}\big)\cup\{j'\}$,
comparing the log-likelihoods under $\widehat{S}$ and $\widehat{S}\,'$, we have
\begin{equation*}
    \log{f(x|\widehat{S})} - \log{f(x|\widehat{S}\,')} 
    = \log{f_a(x(j))} + \log{f_0(x(j'))} - \log{f_a(x(j'))} - \log{f_0(x(j))}.
\end{equation*}
By the definition of $L(j)$'s, and the order relations, we obtain
\begin{equation*}
    \log{f(x|\widehat{S})} - \log{f(x|\widehat{S}\,')} 
    = \log{L(j)} - \log{L(j')} \le 0
\end{equation*}
This implies that any estimator that is not $\widehat{S}_{\text{opt}}$ may be improved, and the optimality follows.
\end{proof}

\begin{remark}
In the non-log-concave setting, where we know that thresholding procedures are suboptimal, likelihood thresholding procedures are promising, thanks to Proposition \ref{prop:likelihood-ratio-thresholding}. 
However, in the case where signals have difference sizes, likelihood thresholding procedures are undefined;
in such settings, existence of an optimal procedure is an open problem.

Indeed, in the proof of Proposition \ref{prop:likelihood-ratio-thresholding}, identical the signal densities are needed so that the relation \eqref{eq:likelihood-ratio-thresholding-proof} holds.
\end{remark}

\subsection{Sub-optimality of data thresholding procedures}

We provide a slightly more general result on the sub-optimality of data thresholding procedures.

\begin{corollary} \label{cor:log-convex}
Consider the additive error model \eqref{eq:model-additive}.
Let the errors $\epsilon$ be independent with common distribution $F$.
Let each of the $s$ signals be located on $\{1,\ldots,p\}$ uniformly at random with equal magnitude $0<\delta<\infty$.
% , independently with probability $p_1,\ldots,p_s$.
% , {\color{red} so that we have $f_a = p_1f_1 + p_2f_2 + \ldots + p_sf_s$.}
If errors $\epsilon$ are iid with density $f$ log-convex on $[K, +\infty)$, then whenever $j\in\widehat{S}_{\text{opt}}$ for some $x(j) > K+\delta$, we must have $j'\in\widehat{S}_{\text{opt}}$ for all $j'$ such that $K+\delta \le x(j') < x(j)$.
\end{corollary} 

Specifically, if there are $m$ observations exceeding $K+\delta_s$, with $m>s$, then the top $m-s$ observations will \emph{not} be included in the optimal estimator $\widehat{S}_{\text{opt}}$.
This shows that, in the case when the errors have super-exponential tails, the optimal procedures are in general \emph{not} data thresholding.

\begin{proof}[Proof of Corollary \ref{cor:log-convex}]
Since the alternative $f_{a}(t) = {f(t-\delta)}$ are log-convex on $[K+\delta, \infty)$, by Relation \eqref{eq:concavity-implies-MLR} in the proof of Lemma \ref{lemma:MLR-log-concavity} and appealing to log-convexity (rather than log-concavity), the likelihood ratio
$$
L(j) := \frac{f_a(x(j))}{f_0(x(j))} %= \sum_{i=1}^s \frac{p_if_{i}(x(j))}{f_0(x(j))},
$$
is decreasing in $x(j)$ on $[K+\delta, \infty)$.
The proof is complete by applying Proposition \ref{prop:likelihood-ratio-thresholding}.
\end{proof}