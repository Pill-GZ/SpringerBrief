The proliferation of information technology has enabled us to collect and consume huge volumes of data at unprecedented speeds and at very low costs.
This convenient access to data gave rise to a fundamentally different way of pursuing scientific questions.
In contrast with the traditional hypothesis--experiment--analysis cycle where data are collected from the experiments, nowadays abundant data are often available before specific questions are even formulated. 
Such data can be used for not just evaluating hypotheses, but also for \emph{generating}, and \emph{selecting} the hypotheses to pursue. 
As a result, {multiple testing} --- where a large number of hypotheses are formulated and screened for their plausibility simultaneously --- has become a staple of modern data-driven studies.

An archetypal example of multiple testing problems is genetic association studies \citep{bush2012genome}.
In these studies, scientists test hypotheses relating each of the hundreds of thousands of genetic marker locations to phenotypic traits of interest.
For a phenotypic trait on which we have little prior knowledge, we cannot simply test for association on one or a few specific genetic locations, as there are often not enough empirical evidence or biological theory to pin point these genetic locations in the first place.
Rather, the goal here is to select the set of most promising genetic markers from a large number of candidate locations for subsequent investigation.

Another example of multiple testing problems arise in cybersecurity, where millions of IP addresses are monitored in real time.
In this engineering application, statistics are collected and tests are performed for each IP address, in an attempt to locate the IP addresses with anomalous network activities, so that and malicious traffic and volumetric attacks can be filtered to protect end users of network services \citep{kallitsis2016amon}.
Similar to the genetic application above, we use data to search over candidate IP addresses and identify locations of interest.

We are motivated very much by these examples to study {high-dimensional} multiple testing problems where a large number of hypotheses are tested simultaneously.
% This high multiplicity of tests brings along a multitude of challenges, and has been a subject of extensive study in recent years.
In the rest of the introduction, we shall more review the main objectives of high-dimensional multiple testing, and elaborate on these objectives with two classes of data models in the context of various applications.

\section{The additive error model}

Consider the canonical signal-plus-noise model where the observation $x$ is a high-dimensional vector in $\R^p$,
\begin{equation} \label{eq:model-additive}
    x(i) = \mu(i) + \epsilon(i), \quad i=1,\ldots,p.
\end{equation}
The signal, $\mu = (\mu(i))_{i=1}^p$, is a vector with $s$ non-zero components supported on the set $S=\{i:\mu(i)\neq 0\}$; the second term $\epsilon$ is a random error vector. 
The goal of high-dimensional statistics is usually two-fold: \\

\begin{enumerate}
\item [{\rm I.}] \emph{Signal detection}: to detect the presence of non-zero components in $\mu$. That is, to test the global hypothesis $\mu = 0$. \\

\item[{\rm II.}] \emph{Support recovery}: to estimate the support set $S$. This is also sometimes referred to as the \emph{support estimation} or \emph{signal identification} problem.
\end{enumerate}
\medskip
To illustrate, in the engineering application of cybersecurity, Internet service providers (ISP) routinely monitor a large number of network traffic streams to determine 
if there are abnormal surges, blackouts, or other types of anomalies.  The data vector $x$ could represent, for example, incoming traffic volumes to each server 
node, internet protocol (IP) address, or port that the ISP monitors.  In this case, the vector $\mu$ represents the average traffic volumes in each of the streams 
under normal operating conditions, and $\epsilon$'s -- the fluctuations around these normal levels of traffic.  
% The ISPs collect, for example, incoming traffic volumes to each server to determine if there are abnormal surges or blackouts.
The signal detection problem in this context is then equivalent to determining if there are \emph{any} anomalies among all data streams, and the support recovery problem is equivalent to \emph{identifying} the streams experiencing anomalies.  
%The dimension of $x$ (number of streams) is typically very large, while the non-trivial 
%signal is sparse (relatively few data streams experience anomalies).
Similar questions of signal detection and support recovery are pursued in large-scale microarray experiments \citep{dudoit2003multiple},
brain imaging and fMRI analysis \citep{nichols2003controlling}, and numerous other anomaly detection applications.

A common theme in such applications is that the errors are {\em correlated}, and that the signal vectors are believed to be {\em sparse}: the number of non-zero
 (or large) components in $\mu$ is small compared to the number of tests performed.  In the cybersecurity context, while a very large number of data streams are 
 monitored, typically only just a few of them will be experiencing problems at any time, barring large-scale outages or distributed denial of service attacks.
Under such sparsity assumptions, it is natural to ask if and when one can reliably {(1)} detect the signals, and {(2)} recover the support set $S$.
% Further, one would like to characterize the statistical procedures that achieve efficient detection and support recovery, whenever such goals become possible to achieve.
In this text, we explore both the \emph{detection} and the \emph{support recovery} problems.  More precisely, we are interested in the 
theoretical feasibility of both problems, and seek minimal conditions under which these problems can be consistently solved in large 
dimensions.


\medskip

Model \eqref{eq:model-additive} is simple yet ubiquitous.
Consider the linear model
\begin{equation*}
 Y = X\mu + \xi,
\end{equation*}
where $\mu$ is a $p$-dimensional vector of regression coefficients of interest to be inferred from observations of $X$ and $Y$.
If the design matrix $X$ is of full column rank\footnote{This, of course, requires that we have more samples than dimensions, i.e., $n>p$. Nevertheless, multiplicity of tests is still present when $p$ itself is large -- the multiple testing problem is by no means exclusive to situations where $p\gg n$.}, then the ordinary least squares (OLS) estimator of $\mu$ can be formed 
\begin{equation} \label{eq:OLS}
    \widehat{\mu} = \left(X'X\right)^{-1}X'Y = \mu + \epsilon,
\end{equation}
where $\epsilon := (X'X)^{-1}X'\xi$. 
% \stilian{R2, Typo 2 -- \fbox{DISCUSS} }{I see no problems here except that we are not in the high-dimensional regime $p\gg n$. If we were to address that, we need to expand the discussion. 1) Talk about sparsity of $\mu$ 2) Introduce the assumption that if ${\rm supp}(\mu) = r <n$, then every $r$ of the $p$ columns of $X$ must be linearly independent -- is there a name for such a condition in the literature? We know it happens with probability one if $X$ has independent, say Gaussian, entries.  3) Then, it seems clear that the support of $\mu$ will be identifiable if we replace the inverse with a Moore-Penrose inverse. BTW, an intriguing question: Given a design matrix $X$, consider the quantity:
% $$
% c_r(X) :=\min_{w\subset \{1,\cdots,p\},\ |w| = r} {\| (X_w' X_w)^{-1}\| \over \| X_w'X_w\| },
% $$
% where the minimum is taken over all subsets of indices $w$ with cardinality $r<n$ and $X_w:= (x_{i,j})_{j\in w}$ is the sub-matrix of $X$ obtained by selecting the columns indexed by the elements in $w\subset\{1,\cdots,p\}$.  Intuitively, $c_r(X)$ gives us the overall difficulty of identifying a sparse signal $\mu$ with support of size $r$. \fbox{Going off on a tangent here...}}
Hence we recover the generic problem \eqref{eq:model-additive}. 
Signal detection is therefore equivalent to the problem of testing the global null model, and support recovery problem corresponds to the fundamental problem of variable selection.

Note that the components of the observation vector $x$ (and equivalently, the noise $\epsilon$) in \eqref{eq:model-additive} need not be independent. 
In the linear regression example, even when the components of the noise term $\xi$ are independent, those of the OLS estimator \eqref{eq:OLS} need not be, except in the case of orthogonal designs.
Indeed, in practice, independence is the exception rather than the rule.
Therefore, a general theory of feasibility must address the role of the {\em error dependence} structure in such testing and support estimation 
problems.
It is also important to identify practical and/or optimal procedures that attain the performance limits in independent as well as dependent cases, as soon as the problems become theoretically feasible.  We address both themes in this text.


\section{Genome-wide association studies and the chi-square model}
\label{sec:motivation-chisq}

The second data model we analyze is the high-dimensional chi-square model,
\begin{equation} \label{eq:model-chisq}
    %x(i) \distras{\mathrm{ind.}} \chi_\nu^2\left(\lambda(i)\right), \quad i=1,\ldots,p.
    x(i) \sim \chi_\nu^2\left(\lambda(i)\right), \quad i=1,\ldots,p,
\end{equation}
where the data $x(i)$'s follow independent (non-central) chi-square distributions with $\nu$ degrees of freedom and non-centrality parameter $\lambda(i)$.

% While the additive error models are frequently featured in the literature, the chi-square models need some introduction.

Model \eqref{eq:model-chisq} is motivated by large-scale categorical variable screening problems, typified by \ac{GWAS}
% genome-wide association studies (GWAS) 
where millions of genetic factors are examined for their potential influence on phenotypic traits.
% We briefly introduce the scientific background next, to provide some context.

% introduced through the language of GWAS next.
% These categorical covariate screening problems naturally give rise to the high-dimensional chi-square models, introduced through the language of GWAS next.
% 
% Broadly speaking, GWAS aim to discover genetic variations that are linked to traits or diseases of interested, by testing for associations between the subjects' genetic compositions and their phenotypes.
In a GWAS with a case-control design, a total of $n$ subjects are recruited,  consisting of $n_1$ subjects possessing some defined traits, and $n_2$ subjects without the traits serving as controls.
The genetic compositions of the subjects are then examined for variations known as \ac{SNP} at an array of $p$ genomic marker locations, and compared between the case and the control group.
These physical traits are commonly referred to as \emph{phenotypes}, and the genetic variations are known as \emph{genotypes}.

Focusing on one specific genomic location, the counts of observed genotypes, if two variants are present, can be tabulated as follows.
% \begin{table}[ht]
% \centering
\begin{center}
    \begin{tabular}{cccc}
    \hline
    & \multicolumn{2}{c}{Genotype} & \\
    \cline{2-3}
    \# Observations & Variant 1 & Variant 2 & Total by phenotype \\
    \hline
    Cases & $O_{11}$ & $O_{12}$ & $n_1$ \\
    Controls & $O_{21}$ & $O_{22}$ & $n_2$ \\
    \hline
    \end{tabular}
    % \caption{Tabulated counts of genotype-phenotype combinations in a genetic association test.}
\end{center}
% \end{table}
Researchers test for associations between the genotypes and phenotypes using, for example, the Pearson chi-square test with statistic
\begin{equation} \label{eq:chisq-statistic}
    x = \sum_{j=1}^2 \sum_{k=1}^2 \frac{(O_{jk} - E_{jk})^2}{E_{jk}},
\end{equation}
where ${E}_{jk} = (O_{j1}+O_{j2})(O_{1k}+O_{2k})/n$.
% are the expected number of observations under the null.
%E_{jk} = \Big(\sum_{l}O_{jl}\Big)\Big(\sum_{l}O_{lk}\Big)\Big/n.

Under the mild assumption that the counts $O_{jk}$'s follow a multinomial distribution (or a product-binomial distribution, if we decide to condition on one of the marginals), the statistic $x$ in \eqref{eq:chisq-statistic} can be shown to have an approximate $\chi^2(\lambda)$ distribution with $\nu=1$ degree of freedom at large sample sizes (see, e.g., classical results in \citet{ferguson2017course} and
\cite{agresti2018introduction}). 
Independence between the genotypes and phenotypes would imply a non-centrality parameter $\lambda$ value of zero; if dependence exists, we would have a non-zero $\lambda$ where its value depends on the underlying multinomial probabilities.
More generally, if we have a $J$ phenotypes and $K$ genetic variants, assuming a $J\times K$ multinomial distribution, the statistic will follow approximately a $\chi^2_{\nu}(\lambda)$ distribution with $\nu = (J-1)(K-1)$ degrees of freedom, when sample sizes are large.

The same asymptotic distributional approximations also apply to the likelihood ratio statistic, and many other statistics under slightly different modeling assumptions \citep{gao2019upass}.
These association tests are performed at each of the $p$ SNP marker locations throughout the whole genome, and we arrive at $p$ statistics having approximately (non-central) chi-square distributions, $\chi_{\nu(i)}^2\left(\lambda(i)\right)$, for $i=1,\ldots,p$,
% \begin{equation} \label{eq:model-chisquare-approx}
%     x(i) \mathrel{\dot\sim} \chi_{\nu(i)}^2\left(\lambda(i)\right), \quad i=1,\ldots,p,
% \end{equation}
where $\lambda = (\lambda(i))_{i=1}^p$ is the $p$-dimensional non-centrality parameter.
%, with $\lambda(i)=0$ indicating independence of the $i$-th SNP with the outcomes, and $\lambda(i)\neq0$ indicating associations.

Although the number of tested genomic locations $p$ can sometimes exceed $10^5$ or even $10^6$, it is often believed that only a small set of genetic locations have tangible influences on the outcome of the disease or the trait of interest.
Under the stylized assumption of sparsity, $\lambda$ is assumed to have $s$ non-zero components, with $s$ being much smaller than the problem dimension $p$. 
The goal of researchers is again two-fold: (1) to test if $\lambda(i)=0$ for all $i$, and (2) to estimate the set $S=\{i:\lambda(i)\neq 0\}$.
In other words, we look to first determine if there are \emph{any} genetic variations associated with the disease; and if there are associations, we want to locate them.

\medskip

The chi-square model \eqref{eq:model-chisq} also plays an important role in analyzing variable screening problems under omnidirectional alternatives.
A primary example is multiple testing under two-sided alternatives in the additive error model \eqref{eq:model-additive} where the errors $\epsilon$ are assumed to have standard normal distributions.

Under two-sided alternatives, unbiased test procedures call for rejecting the hypothesis $\mu(i)=0$ at locations where observations have large absolute values, or equivalently, large squared values.
Taking squares on both sides of \eqref{eq:model-additive}, and we arrive at Model \eqref{eq:model-chisq} with non-centrality parameters $\lambda(i) = \mu^2(i)$ and degree-of-freedom parameter $\nu =1$.
In this case, the support recovery problem is equivalent to locating the set of observations with mean shifts, $S=\{i:\mu(i)\neq 0\}$, where the mean shifts could take place in both directions.

Therefore, a theory for the chi-square model \eqref{eq:model-chisq} naturally lends itself to the study of two-sided alternatives in the Gaussian additive error model \eqref{eq:model-additive}.
In comparing such results with existing theory on one-sided alternatives, we will be able to quantify if, and how much of a price has to be paid for the additional uncertainty when we have no prior knowledge on the direction of the signals.

% In many applications, of course, restricting ourselves to one-sided tests is unrealistic.
% For example, in fMRI studies, the interest is in \emph{both} regions where average brain activities {increase} and where they {decrease}, when comparing the case group to the controls \citep{narayan2015two}. 
% In the challenging application of anomaly detection on Internet traffic streams, millions of IP addresses need to be scanned in real time to identify \emph{both} volumetric attacks and blackouts \citep{kallitsis2016amon}.
% Indeed, omnidirectional tests are the more natural choice in so-called discovery sciences where little to no prior knowledge is available.

\section{Contents}

Important notions and definitions in high-dimensional testing problems are recalled in Chapter \ref{chap:background}. 
We review related literature as well as key concepts and technical results used in our subsequent analyses.

In Chapter \ref{chap:phase-transitions} we study the sparse signal detection and support recovery problems for the additive error model \eqref{eq:model-additive} when components of the noise term $\epsilon$ are independent standard Gaussian random variables.
In particular, we point out several new \emph{phase transitions} in signal detection problems, and provide a unified account of recently discovered phase transitions in support recovery problems.
These result show that as the dimension $p\to\infty$, the tasks of detecting the existence of signals, or identifying the support set $S$ are either doable or impossible depending on the sparsity and signal sizes of the problems.
We also identify commonly used procedures that attain the performance limits in both detection and support recovery problems.

Both the Gaussianity assumption and the independence assumption are relaxed in Chapter \ref{chap:exact-support-recovery}.
Established are the necessary and sufficient conditions for exact support recovery in the high-dimensional asymptotic regime.
This is a major theoretical contribution of our approach, which solves and expands on open problems in the recent literature 
(see \cite{butucea2018variable, gao2018fundamental}).
The analysis of support recovery problem is intimately related to a \emph{concentration of maxima} phenomena in the analysis of extremes.
The latter concept is key to understanding the role played by dependence in the phase transition phenomena of high-dimensional testing problems.
Using this probabilistic concept, we establish minimax optimality results that hold for a very large class of dependence structures.

The dependence condition defined by the concentration of maxima  concepts is further demystified in Chapter \ref{chap:URS} for Gaussian errors.
We offer a complete characterization of the concentration of maxima phenomenon, known as uniform relative stability, in terms of the covariance structures of the Gaussian arrays.
This result may be of independent interest since it relates to the so-called \emph{superconcentration} phenomenon coined by \cite{chatterjee2014superconcentration}.
See also, \cite{gao2018fundamental} and \citet*{kartsioukas2019rate}.

Chapter \ref{chap:GWAS} returns to high-dimensional multiple testing problems, and study the chi-square model \eqref{eq:model-chisq} inspired by the marginal association screening problems.
We establish four new phase-transition-type results in the chi-square model, and illustrate their practical implications in the GWAS application.
Our theory enables us to explain the long-standing empirical observation that small perturbations in the frequency and penetrance of genetic variations lead to drastic changes in the discoverability in genetic association studies.
% We also provide a user-friendly web-based software tool for planning, and reviewing, genome-wide association studies, published in \cite{gao2019upass}.



