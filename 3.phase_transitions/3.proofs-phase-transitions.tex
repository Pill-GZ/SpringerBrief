
We first recall some basic properties of the Gaussian distribution in Section \ref{sec:Gaussian-distributions}.
Section \ref{suppsec:BH-monotonicity} states and proves an interesting property of the \ac{BH} procedure which may be of independent interest.
Results on the signal detection problem (Theorem \ref{thm:detection-optimality}) are proved in Section \ref{subsec:proof-additive-error-detection-boundaries}, and the phase transition results on the support recovery problems (Theorems \ref{thm:Gaussian-error-exact-boundary} through \ref{thm:Gaussian-error-approx-exact-boundary}) are shown in Sections \ref{subsec:proof-additive-error-approx-boundaries} and \ref{subsec:proof-additive-error-mix-boundaries}.


\section{Auxiliary facts of Gaussian distributions}
\label{sec:Gaussian-distributions}
\input{3.phase_transitions/3.Gaussian-distributions.tex}

\section{Monotonicity of the Benjamini-Hochberg procedure}
\label{suppsec:BH-monotonicity}
\input{3.phase_transitions/3.BH_monotonicity.tex}

\section{Proof of Theorem \ref{thm:detection-optimality}}
\label{subsec:proof-additive-error-detection-boundaries}







\section{Proof of Theorem \ref{thm:Gaussian-error-approx-boundary}}
\label{subsec:proof-additive-error-approx-boundaries}

We first show the necessary condition. 
That is, when $\overline{r} < \beta$, no thresholding procedure is able to achieve approximate support recovery.
The arguments are similar to that in Theorem 1 of \cite{arias2017distribution}, although we allow for unequal signal sizes. 

\begin{proof}[Proof of necessary condition in Theorem \ref{thm:Gaussian-error-approx-boundary}]
Denote the distributions of $\mathrm{N}(0,1)$, $\mathrm{N}(\underline{\Delta}, 1)$, and $\mathrm{N}((\overline{\Delta}, 1)$ as $F_0$, $F_{\underline{a}}$, and $F_{\overline{a}}$ respectively.

% We first show the necessary condition, i.e., when $\overline{r}<\beta$, approximate support recovery cannot be achieved with any thresholding procedure.
% In particular, we show that the liminf of the sum of FDP and NDP is at least 1.

Recall that thresholding procedures are of the form
$$
\widehat{S}_p = \left\{i\,|\,x(i) > t_p(x)\right\}.
$$
Denote $\widehat{S} := \left\{i\,|\,x(i) > t_p(x)\right\}$, and $\widehat{S}(u) := \left\{i\,|\,x(i) > u\right\}$.
For any threshold $u\ge t_p$ we must have $\widehat{S}(u)\subseteq\widehat{S}$, and hence
\begin{equation} \label{eq:approx-boundary-proof-FDP-Gaussian}
    \text{FDP} := \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}|} \ge \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}\cup{S}|} = \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}\setminus{S}| + |S|} \ge
    \frac{|\widehat{S}(u)\setminus{S}|}{|\widehat{S}(u)\setminus{S}| + |S|}.
\end{equation}
On the other hand, for any threshold $u\le t_p$ we must have $\widehat{S}(u)\supseteq\widehat{S}$, and hence
\begin{equation} \label{eq:approx-boundary-proof-NDP-Gaussian}
    \text{NDP} := \frac{|{S}\setminus\widehat{S}|}{|{S}|} \ge 
    \frac{|{S}\setminus\widehat{S}(u)|}{|{S}|}.
\end{equation}
Since either $u\ge t_p$ or  $u\le t_p$ must take place, putting \eqref{eq:approx-boundary-proof-FDP-Gaussian} and \eqref{eq:approx-boundary-proof-NDP-Gaussian} together, we have
\begin{equation} \label{eq:approx-boundary-proof-converse-1-Gaussian}
    \text{FDP} + \text{NDP} 
    \ge \frac{|\widehat{S}(u)\setminus{S}|}{|\widehat{S}(u)\setminus{S}|+|{S}|} \wedge \frac{|{S}\setminus\widehat{S}(u)|}{|{S}|},
\end{equation}
for any $u$.
Therefore it suffices to show that for a suitable choice of $u$, the RHS of \eqref{eq:approx-boundary-proof-converse-1-Gaussian} converges to 1 in probability; the desired conclusion on FDR and FNR follows by the dominated convergence theorem.

Let $t^* = \sqrt{2q\log{p}}$ for some fixed $q$, we obtain an estimate of the tail probability by Mill's ratio \eqref{eq:Mills-ratio}, 
\begin{equation}
    \overline{F_0}(t^*) 
    \sim \frac{1}{t^*}\phi(t^*)
    = \frac{1}{2\sqrt{\pi q\log{p}}} p^{-q}, \label{eq:approx-boundary-proof-null-tail-prob-Gaussian}
\end{equation}
where $a_p\sim b_p$ is taken to mean $a_p/b_p\to 1$.
Observe that $|\widehat{S}(t^*)\setminus{S}|$ has distribution $\text{Binom}(p-s, \overline{F_0}(t^*))$ where $s=|S|$, denote $X = X_p := {|\widehat{S}(t^*)\setminus{S}|}/{|S|}$, and we have 
$$
\mu := \E\left[X\right] = \frac{(p-s)\overline{F_0}(t^*)}{s},
\quad \text{and} \quad
\var\left(X\right) = \frac{(p-s)\overline{F_0}(t^*){F_0}(t^*)}{s^2} \le \mu/s.
$$
Therefore for any $M>0$, we have, by Chebyshev's inequality,
\begin{equation}
    \P\left[X < M\right] 
    \le \P\left[\left|X-\mu\right| > \mu - M\right]
    \le \frac{\mu/s}{(\mu-M)^2}
    = \frac{1/(\mu s)}{(1-M/\mu)^2}. \label{eq:approx-boundary-proof-converse-2-Gaussian}
\end{equation}
Now, from the expression of $\overline{F_0}(t^*)$ in \eqref{eq:approx-boundary-proof-null-tail-prob-Gaussian}, we obtain
$$
\mu = (p^\beta - 1)\overline{F_0}(t^*) \sim \frac{1}{2\sqrt{\pi q\log{p}}} p^{\beta-q}.
$$
Since $\overline{r}<\beta$, we can pick $q$ such that $\overline{r}<q<\beta$. 
In turn, we have $\mu \to\infty$, as $p\to\infty$.
Therefore the last expression in \eqref{eq:approx-boundary-proof-converse-2-Gaussian} converges to 0, and we conclude that $X\to\infty$ in probability, and hence
\begin{equation} \label{eq:approx-boundary-proof-converse-3-Gaussian}
\frac{|\widehat{S}(t^*)\setminus{S}|}{|\widehat{S}(t^*)\setminus{S}|+|{S}|} 
= \frac{X}{X+1} \to 1 \quad \text{in probability}.
\end{equation}

On the other hand, we show that with the same choice of $u = t^*$, we have,
\begin{equation} \label{eq:approx-boundary-proof-converse-4-Gaussian}
    \frac{|{S}\setminus\widehat{S}(t^*)|}{|{S}|}\to 1 \quad \text{in probability}.
\end{equation}
By the stochastic monotonicity of Gaussian location family \eqref{eq:stochastic-monotonicity-Gaussian}, we have the following lower bound for the probability of missed detection for each signal $\mu(i)$, $i\in S$, 
\begin{equation} \label{eq:approx-boundary-proof-converse-5-Gaussian}
    \P[\mathrm{N}(\mu(i), 1) \le t^*] \ge F_{\overline{a}}(t^*).
\end{equation}
Since $|{S}\setminus\widehat{S}(t^*)|$ can be written as the sum of $s$ independent Bernoulli random variables,
\begin{equation*}
    |{S}\setminus\widehat{S}(t^*)| = \sum_{i\in S} \mathbbm{1}_{(-\infty, t^*]}(x(i)),
\end{equation*}
using with \eqref{eq:approx-boundary-proof-converse-5-Gaussian}, we conclude that $|{S}\setminus\widehat{S}(t^*)| \stackrel{\mathrm{d}}{\ge} \text{Binom}(s, {F_{\overline{a}}}(t^*))$.
Finally, we know that ${F_{\overline{a}}}(t^*)$ converges to 1 by our choice of diverging $t^*$, and the necessary condition is shown.
\end{proof}

We now turn to the sufficient condition. 
That is, when $\underline{r} > \beta$, the Benjamini-Hochberg procedure with slowly vanishing FDR levels achieves asymptotic approximate support recovery.

\begin{proof}[Proof of the sufficient condition in Theorem \ref{thm:Gaussian-error-approx-boundary}]
The FDR vanishes by our choice of $\alpha$ and the FDR-controlling property of the BH procedure \citep{benjamini1995controlling}.
It only remains to show that FNR also vanishes.

To do so we compare the FNR under the alternative specified in Theorem \ref{thm:Gaussian-error-approx-boundary} to one with all of the signal sizes equal to $\underline{\Delta}$.
By Lemma \ref{lemma:monotonicity-BH-procedure}, it suffices to show that the FNR under the BH procedure in this setting vanishes.
Let $x(i)$ be vectors of independent observations with $p-s$ nulls having standard Gaussian distributions, and $s$ signals having $\mathrm{N}(\underline{\Delta}, 1)$ distributions.

Denote the null and the alternative distributions as $F_0$ and $F_{a}$ respectively.
Let $\widehat{G}$ denote the empirical survival function as in \eqref{eq:empirical-tail-distribution}.
Define the empirical survival functions for the null part and signal part
\begin{equation} \label{eq:empirical-survival-null-signal-Gaussian}
    \widehat{W}_\text{null}(t) = \frac{1}{p-s}\sum_{i\not\in S}\mathbbm{1}\{x(i) \ge t\},
    \quad
    \widehat{W}_\text{signal}(t) = \frac{1}{s}\sum_{i\in S}\mathbbm{1}\{x(i) \ge t\},
\end{equation}
where $s=|S|$, so that
$$
\widehat{G}(t) = \frac{p-s}{p}\widehat{W}_\text{null}(t) + \frac{s}{p}\widehat{W}_\text{signal}(t).
$$

We need the following result to describe the deviations of the empirical distributions.
\begin{lemma}[Theorem 1 of \citet{eicker1979asymptotic}] \label{lemma:empirical-process}
Let $Z_1,\ldots,Z_k$ be iid with continuous survival function $Q$.
Let $\widehat{Q}_k$ denote their empirical survival function and define 
$\xi_k = \sqrt{2\log{\log{(k)}}/k}$ for $k \ge 3$. 
Then
$$
\frac{1}{\xi_k}\sup_z\frac{|\widehat{Q}_k(z) - Q(z)|}{\sqrt{Q(z)(1 - Q(z))}} \to 1,
$$
in probability as $k \to \infty$.
In particular,
$$
\widehat{Q}_k(z) = Q(z) + O_\P\left(\xi_k\sqrt{Q(z)(1 - Q(z))}\right),
$$
uniformly in z.
\end{lemma}

Apply Lemma \ref{lemma:empirical-process} to the two summands in $\widehat{G}$, we obtain
$\widehat{G}(t) = G(t) + \widehat{R}(t)$,
where 
\begin{equation} \label{eq:empirical-process-mean-Gaussian}
    G(t) = \frac{p-s}{p}\overline{F_0}(t) + \frac{s}{p}\overline{F_a}(t),
\end{equation}
and 
\begin{equation} \label{eq:empirical-process-residual-Gaussian}
    \widehat{R}(t) = O_\P\left(\xi_p\sqrt{\overline{F_0}(t)F_0(t)} + \frac{s}{p}\xi_s\sqrt{\overline{F_a}(t)F_a(t)}\right),
\end{equation}
uniformly in $t$.

Recall (see proof of Lemma \ref{lemma:monotonicity-BH-procedure}) that the BH procedure is the thresholding procedure with threshold set at 
\begin{equation} \label{eq:approx-boundary-proof-tau-Gaussian}
    \tau = \inf\{t\,|\,\overline{F_0}(t)\le\alpha\widehat{G}(t)\}. 
    %= \min\{t\,|\,\overline{F_0}(t)=\alpha\widehat{G}(t)\}.
\end{equation}
The NDP may also be re-written as 
$$
\text{NDP} = \frac{|{S}\setminus\widehat{S}|}{|{S}|} = \frac{1}{s}\sum_{i\in S}\mathbbm{1}\{x(i) < \tau\} = 1 - \widehat{W}_\text{signal}(\tau),
$$
so that it suffices to show that 
\begin{equation} \label{eq:approx-boundary-proof-sufficient-1-Gaussian}
    \widehat{W}_\text{signal}(\tau)\to 1
\end{equation} in probability.
Applying Lemma \ref{lemma:empirical-process} to $\widehat{W}_\text{signal}$, we know that 
$$
\widehat{W}_\text{signal}(\tau) = \overline{F_a}(\tau) + O_\P\left(\xi_s\sqrt{\overline{F_a}(\tau)F_a(\tau)}\right) = \overline{F_a}(\tau) + o_\P(1).
$$
So it suffices to show that $F_a(\tau)\to 0$ in probability.
Now let $t^* = \sqrt{2q\log(p)}$ for some $q$ such that $\beta<q<\underline{r}$.
We have 
\begin{equation} \label{eq:approx-boundary-proof-sufficient-2-Gaussian}
    F_a(t^*) 
    = \Phi(t^* - \underline{\Delta}) 
    = \Phi(\sqrt{2(q - \underline{r})\log{p}}) \to 0. 
\end{equation}
Hence in order to show \eqref{eq:approx-boundary-proof-sufficient-1-Gaussian}, it suffices to show 
\begin{equation} \label{eq:approx-boundary-proof-sufficient-3-Gaussian}
    \P\left[\tau \le t^*\right] \to 1.
\end{equation}
By \eqref{eq:empirical-process-mean-Gaussian}, the mean of the empirical process $\widehat{G}$ evaluated at $t^*$ is
\begin{equation} \label{eq:approx-boundary-proof-sufficient-4-Gaussian}
    G(t^*) = \frac{p-s}{p}\overline{F_0}(t^*) + \frac{s}{p}\overline{F_a}(t^*).
\end{equation}
The first term, using Relation \eqref{eq:approx-boundary-proof-null-tail-prob-Gaussian}, is asymptotic to $p^{-q}L(p)$, where $L(p)$ is the logarithmic term in $p$.
The second term, since $\overline{F_a}(t^*)\to 1$ by Relation \eqref{eq:approx-boundary-proof-sufficient-2-Gaussian}, is asymptotic to $p^{-\beta}$.
Therefore, $G(t^*) \sim p^{-q}L(p) + p^{-\beta} \sim p^{-\beta}$, since 
$p^{\beta-q}L(p)\to0$ where $q>\beta$.

The fluctuation of the empirical process at $t^*$, by Relation \eqref{eq:empirical-process-residual-Gaussian}, is 
\begin{align*}
    \widehat{R}(t^*) 
    &= O_\P\left(\xi_p\sqrt{\overline{F_0}(t^*)F_0(t^*)} + \frac{s}{p}\xi_s\sqrt{\overline{F_a}(t^*)F_a(t^*)}\right)\\
    &= O_\P\left(\xi_p\sqrt{\overline{F_0}(t^*)}\right) + o_\P\left(p^{-\beta}\right).
\end{align*}
By \eqref{eq:approx-boundary-proof-null-tail-prob-Gaussian} and the expression for $\xi_p$, the first term is $O_\P\left(p^{-(q+1)/2}L(p)\right)$ where $L(p)$ is a poly-logarithmic term in $p$.
Since $\beta<\min\{q,1\}$, we have $\beta<(q+1)/2$, and hence $\widehat{R}(t^*) = o_\P(p^{-\beta})$.

Putting the mean and the fluctuation of $\widehat{G}(t^*)$ together, we obtain
$$
\widehat{G}(t^*) = G(t^*) + \widehat{R}(t^*) \sim_\P G(t^*) \sim p^{-\beta},
$$
and therefore, together with \eqref{eq:approx-boundary-proof-null-tail-prob-Gaussian}, we have
$$
\overline{F_0}(t^*)/\widehat{G}(t^*) = p^{\beta-q}L(p)(1+o_{\P}(1)),
$$
which is eventually smaller than the FDR level $\alpha$ by the assumption \eqref{eq:slowly-vanishing-error} and the fact that $\beta<q$.
That is, 
$$
\P\left[\overline{F}_0(t^*) / \widehat{G}(t^*) < \alpha\right] \to 1.
$$
By definition of $\tau$ (recall \eqref{eq:approx-boundary-proof-tau-Gaussian}), this implies that $\tau \le t^*$ with probability tending to 1, and \eqref{eq:approx-boundary-proof-sufficient-3-Gaussian} is shown.
The proof for the sufficient condition is complete.
\end{proof}










\section{Proof of Theorems \ref{thm:Gaussian-error-exact-approx-boundary} and \ref{thm:Gaussian-error-approx-exact-boundary}}
\label{subsec:proof-additive-error-mix-boundaries}


Proof of Theorem \ref{thm:Gaussian-error-exact-approx-boundary} uses ideas from the proof of Theorem \ref{thm:Gaussian-error-approx-boundary} and is substantially shorter.


\begin{proof}[Proof of Theorem \ref{thm:Gaussian-error-exact-approx-boundary}]
We first show the sufficient condition. 
Vanishing FWER is guaranteed by the properties of the procedures, and we only need to show that FNR also goes to zero. 
Similar to the proof of Theorem \ref{thm:Gaussian-error-approx-boundary}, it suffices to show that
\begin{equation} \label{eq:additive-error-exact-approx-boundary-proof-sufficient-1}
    \text{NDP} = 1 - \widehat{W}_\text{signal}(t_p) \to 0,
\end{equation}
where $t_p$ is the threshold of Bonferroni's procedure.

Since $\alpha$ vanishes slowly (see Definition \ref{eq:slowly-vanishing-error}), for any $\delta>0$, we have $p^{-\delta}=o(\alpha)$.
Therefore, we have $-\log\alpha\le\delta\log{p}$ for large $p$, and
\begin{equation*} 
    1 \le \limsup_{p\to\infty}\frac{2\log{p} - 2\log{\alpha}}{2\log{p}} \le 1+\delta,
\end{equation*}
for any $\delta>0$.
Therefore, by the expression for normal quantiles, we know that 
$$
t_p=F^\leftarrow(1-\alpha/p)\sim(2\log{p}-2\log{\alpha})^{1/2} \sim(2\log{p})^{1/2}.
$$

Since $\underline{r}>\widetilde{g}(\beta)=1$, we can pick $q$ such that $1<q<\underline{r}$.
Let $t^* = \sqrt{2q\log{p}}$, we know that $t_p<t_p^*$ for large $p$.
Therefore for large $p$, we have
$$
\widehat{W}_\text{signal}(t_p) \ge \widehat{W}_\text{signal}(t^*) \ge \overline{F_a}(t^*) + o_\P(1),
$$
where $\overline{F_a}$ is the survival function of $\mathrm{N}(\sqrt{2\underline{r}\log{p}}, 1)$; the last inequality follows from the stochastic monotonicity of the Gaussian location family \eqref{eq:stochastic-monotonicity-Gaussian}, and Lemma \ref{lemma:empirical-process}.
Indeed, by our choice of $q<\underline{r}$, we obtain
$$
F_a(t^*) = \Phi\left(\sqrt{2(q-\underline{r})\log{p}}\right)\to0,
$$
and \eqref{eq:additive-error-exact-approx-boundary-proof-sufficient-1} is shown. 
This completes the proof of the sufficient condition.

The proof of the necessary condition follows similar structure as in the proof of Theorem \ref{thm:Gaussian-error-approx-boundary}, and uses the lower bound
\begin{equation} \label{eq:additive-error-exact-approx-boundary-proof-necessary-1}
    \mathrm{FWER}(\mathcal{R}) + \mathrm{FNR}(\mathcal{R}) \ge \P\left[\max_{i\in S^c}x(i)>u\right] \wedge \E\left[\frac{|S\setminus \widehat{S}(u)|}{|S|}\right],
\end{equation}
which holds for any arbitrary thresholding procedure $\mathcal{R}$ and arbitrary real $u\in\R$.

By the assumption that $\overline{r}<\widetilde{g}(\beta)=1$, we can pick $q$ such that $\overline{r}<q<1$ and let $u = t^*=\sqrt{2q\log{p}}$ in \eqref{eq:additive-error-exact-approx-boundary-proof-necessary-1}.
By relative stability of iid Gaussian random variables \eqref{eq:relative-stability-Gaussian-maxima}, we have
\begin{equation} \label{eq:additive-error-exact-approx-boundary-proof-necessary-2}
    \P\left[\frac{\max_{i\in S^c} x(i)}{\sqrt{2\log{p}}} > \frac{t^*}{\sqrt{2\log{p}}}\right] \to 1.
\end{equation}
since the first fraction in \eqref{eq:additive-error-exact-approx-boundary-proof-necessary-2} converges to 1, while the second converges to $q<1$.
Therefore, the first term on the right-hand side of \eqref{eq:additive-error-exact-approx-boundary-proof-necessary-1} converges to 1.

On the other hand, by the stochastic monotonicity of Gaussian location family \eqref{eq:stochastic-monotonicity-Gaussian}, the probability of missed detection for each signal is lower bounded by $\P[Z+\mu(i) \le t^*] \ge F_{\overline{a}}(t^*)$, where $Z$ is a standard Gaussian r.v., and $F_{\overline{a}}$ is the cdf of $\mathrm{N}(\sqrt{2\overline{r}\log{p}}, 1)$.
Therefore, $|{S}\setminus\widehat{S}(t^*)| \stackrel{\mathrm{d}}{\ge} \text{Binom}(s, {F_{\overline{a}}}(t^*))$, and it suffices to show that ${F_{\overline{a}}}(t^*)$ converges to 1.
Indeed,
\begin{equation*}
    {F_{\overline{a}}}(t^*) = \Phi(\sqrt{2(q-\overline{r})\log{p}}) \to 1,
\end{equation*}
by our choice of $q>\overline{r}$.
Hence both quantities in the minimum on the right-hand side of \eqref{eq:additive-error-exact-approx-boundary-proof-necessary-1} converge to 1 in the limit, and the necessary condition is shown.
\end{proof}






\begin{proof}[Proof of Theorem \ref{thm:Gaussian-error-approx-exact-boundary}]
We first show the sufficient condition.
Since FDR control is guaranteed by the BH procedure, we only need to show that the FWNR also vanishes, that is,
\begin{equation} \label{eq:approx-exact-boundary-proof-sufficient-1-Gaussian}
    \P\left[\min_{i\in S}x(i) \ge \tau\right] \to 1,
\end{equation}
where $\tau$ is the threshold for the BH procedure.

By the assumption that $\underline{r}>\widetilde{h}(\beta)=(\sqrt{\beta}+\sqrt{1-\beta})^2$, we have $\sqrt{\underline{r}}-\sqrt{1-\beta}>\sqrt{\beta}$, so we can pick $q>0$, such that 
\begin{equation} \label{eq:approx-exact-boundary-proof-sufficient-2-Gaussian}
\sqrt{\underline{r}}-\sqrt{1-\beta}>\sqrt{q}>\sqrt{\beta}.
\end{equation}
We only need to show that with a specific choice of $t^*=\sqrt{2q\log{p}}$ where
\begin{equation} \label{eq:additive-error-approx-exact-boundary-proof-sufficient-1}
\sqrt{\underline{r}}-\sqrt{1-\beta}>\sqrt{q}>\sqrt{\beta},
\end{equation}
we have both
\begin{equation} \label{additive-error-eq:approx-exact-boundary-proof-sufficient-2}
\P\left[\tau\le t^*\right]\to 1,
\end{equation}
and 
\begin{equation} \label{eq:additive-error-approx-exact-boundary-proof-sufficient-3}
    \P\left[\min_{i\in S}x(i) \ge t^* \right] \to 1,
\end{equation}
so that 
\begin{equation*} 
    \P\left[\min_{i\in S}x(i) \ge \tau\right] \ge 
    % \P\left[\min_{i\in S}x(i) \ge t^* \ge \tau\right] \ge
    \P\left[\min_{i\in S}x(i) \ge t^*,\; t^* \ge \tau\right] \to 1.
\end{equation*}

Relation \eqref{additive-error-eq:approx-exact-boundary-proof-sufficient-2} follows in exactly the same way \eqref{eq:approx-boundary-proof-sufficient-3-Gaussian} did on page  \pageref{eq:approx-boundary-proof-sufficient-3-Gaussian}.

Dividing the left-hand-side in Relation \eqref{eq:additive-error-approx-exact-boundary-proof-sufficient-3} by $\sqrt{2\log{p}}$, we have,
\begin{align*}
    \frac{\min_{i\in S}x(i)}{\sqrt{2\log{p}}} 
    &= \frac{\min_{i\in S}\mu(i)+\epsilon(i)}{\sqrt{2\log{p}}} 
    \stackrel{\mathrm{d}}{\ge} \frac{\sqrt{2\underline{r}\log{p}} + \min_{i\in S}\epsilon(i)}{\sqrt{2\log{p}}} \\
    &\to -\sqrt{1-\beta} + \sqrt{\underline{r}},
\end{align*}
where the last convergence follows from the relative stability of iid Gaussians minima \eqref{eq:relative-stability-Gaussian-minima}. 
On the other hand, ${t^*}/{\sqrt{2\log{p}}}=\sqrt{q}<\sqrt{\underline{r}}-\sqrt{1-\beta}$ by our choice of ${q}$, and Relation \eqref{eq:additive-error-approx-exact-boundary-proof-sufficient-3} follows.


The necessary condition follows from the lower bound
\begin{equation} \label{eq:additive-error-approx-exact-boundary-proof-necessary-1}
    \mathrm{FDR}(\mathcal{R}) + \mathrm{FWNR}(\mathcal{R}) \ge \E\left[\frac{|\widehat{S}(u)\setminus S|}{|\widehat{S}(u)\setminus S| + |S|}\right] \wedge 
    \P\left[\min_{i\in S}x(i)<u\right],
\end{equation}
which holds for any thresholding procedure $\mathcal{R}$ and for arbitrary $u\in\R$.
In particular, we show that both terms in the minimum in \eqref{eq:additive-error-approx-exact-boundary-proof-necessary-1} converge to 1 when we set $u=t^*=\sqrt{2q\log{p}}$ where 
\begin{equation}
\sqrt{\overline{r}}-\sqrt{1-\beta}<\sqrt{q}<\sqrt{\beta}.
\end{equation}

On the one hand, we have,
$$
\frac{\min_{i\in S}x(i)}{\sqrt{2\log{p}}} 
\stackrel{\mathrm{d}}{\le} \frac{\min_{i\in S}\epsilon(i)+\sqrt{2\overline{r}\log{p}}}{\sqrt{2\log{p}}} 
\to \sqrt{\overline{r}}-\sqrt{1-\beta},
$$
by relative stability of iid Gaussians \eqref{eq:relative-stability-Gaussian-minima}. On the other hand, ${t^*}/{\sqrt{2\log{p}}}=\sqrt{q}>\sqrt{\underline{r}}-\sqrt{1-\beta}$ by our choice of ${q}$;
this shows that the second term on the right-hand side of \eqref{eq:additive-error-approx-exact-boundary-proof-necessary-1} converges to 1.

Observe that $|\widehat{S}(t^*)\setminus{S}|$ has distribution $\text{Binom}(p-s, \overline{\Phi}(t^*))$, and define $X = X_p := {|\widehat{S}(t^*)\setminus{S}|}/{|S|}$, we obtain,
% On the other hand, define $ = \E[|S\setminus\widehat{S}(t^*)|/|S|]$,
\begin{align*}
    \mu &:= \E[X] = (p^\beta-1)\overline{\Phi}(t^*) 
    \sim (p^\beta-1)\frac{\phi(t^*)}{t^*} \\
    &\sim \frac{1}{\sqrt{2\pi}}\left(2q\log{p}\right)^{-1/2}p^{\beta-q}\to\infty,
\end{align*}
where the divergence follows from our choice of $q<\beta$.
Using again Relations \eqref{eq:approx-boundary-proof-converse-2-Gaussian} and \eqref{eq:approx-boundary-proof-converse-3-Gaussian}, we conclude that the first term on the right-hand side of \eqref{eq:additive-error-approx-exact-boundary-proof-necessary-1} also converges to 1.
This completes the proof of the necessary condition.
\end{proof}
